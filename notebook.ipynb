{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaabe7791de8e2",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0866b873e18ef1",
   "metadata": {},
   "source": [
    "Dask - это библиотека для параллельных вычислений и масштабирования. Допустим, что Numpy не справляется с объемом данных, который нам нужно будет обработать. В таком случае естественной альтернативой как раз станет Dask. Dask предоставляет возможность работать с данными, которые превышают объем оперативной памяти, и эффективно использовать ресурсы как на локальной машине, так и на кластере. Библиотека позволяет масштабировать код Python с минимальными изменениями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0472af2d568ca8d",
   "metadata": {},
   "source": [
    "## Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29291f71cc75d8d2",
   "metadata": {},
   "source": [
    "[Тык](https://docs.dask.org/en/stable/)\n",
    "\n",
    "[Тык](https://tutorial.dask.org/00_overview.html)\n",
    "\n",
    "[Тык](https://habr.com/ru/companies/otus/articles/759552/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d1de40e22ccbd",
   "metadata": {},
   "source": [
    "## Установка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1454e34d3ca316",
   "metadata": {},
   "source": [
    "Здесь все стандартно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T09:18:47.576563Z",
     "start_time": "2024-12-10T09:18:46.714320Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889c3db692cd4c",
   "metadata": {},
   "source": [
    "# Dask Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b487daee63ae4be",
   "metadata": {},
   "source": [
    "Dask Array — это масштабируемая версия массива NumPy, которая работает с массивами, превышающими объем оперативной памяти, и распределяет вычисления между несколькими ядрами или машинами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550f73cc6ab3f8a",
   "metadata": {},
   "source": [
    "- Dask Array поддерживает API NumPy, что позволяет использовать знакомые функции и методы.\n",
    "\n",
    "- Массивы разбиваются на более мелкие блоки, которые обрабатываются независимо.\n",
    "\n",
    "- Поддерживаются вычисления как на локальной машине, так и в распределенных системах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fa0ba0343c15a",
   "metadata": {},
   "source": [
    "Рассмотрим для начала пример из документации:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43640558f269a226",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c302565998fe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T09:20:15.047610Z",
     "start_time": "2024-12-10T09:18:56.796302Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "# Создаем обычный NumPy массив размером 10x10\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "# Преобразуем NumPy массив в Dask Array с чанками (разбиением) 5x5\n",
    "dask_array = da.from_array(x, chunks=(5, 5))\n",
    "\n",
    "result = dask_array.mean()\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f71c7f6acb2ef",
   "metadata": {},
   "source": [
    "Разберемся, что здесь вообще происходит.\n",
    "\n",
    "Вместо того чтобы обрабатывать весь массив сразу, Dask делит его на чанки, что позволяет работать с большими данными, превышающими оперативную память.\n",
    "Выполнение вычислений параллелизуется, что ускоряет процесс на многопроцессорных системах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e711b16ca80a79",
   "metadata": {},
   "source": [
    "![dask-array](images/dask-array-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d8272913d6ddb",
   "metadata": {},
   "source": [
    "На изображении показано, как Dask Array разбивает большой массив на более мелкие части — чанки. Каждый чанк представляет собой отдельный NumPy массив."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a12fa05a482a5",
   "metadata": {},
   "source": [
    "Пусть нам нужно вычислить индекс растительности NDVI для спутниковых данных, представляющих 10 сцен, каждая из которых состоит из двух каналов: ближний инфракрасный (NIR) и красный (Red). NDVI является важным показателем для анализа растительности, так как он показывает степень активности фотосинтеза в растительных покровах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512dad63391bb196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем массив данных: снимки 10000x10000 (10 сцен, 2 канала: NIR и Red)\n",
    "satellite_data = da.random.random((10, 2, 10_000, 10_000), chunks=(1, 2, 5000, 5000))\n",
    "\n",
    "# Разделяем каналы\n",
    "nir = satellite_data[:, 0, :, :]  # Near Infrared\n",
    "red = satellite_data[:, 1, :, :]  # Red\n",
    "\n",
    "# Вычисляем NDVI\n",
    "ndvi = (nir - red) / (nir + red)\n",
    "\n",
    "print(\"NDVI сцены 1, пиксель [0, 0]:\", ndvi[0, 0, 0].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b8f50183b37",
   "metadata": {},
   "source": [
    "Попробуем сделать то же самое с NumPy (этот код ниже лучше не запускать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc5a90c4993bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем аналогичный массив данных: снимки 10000x10000 (10 сцен, 2 канала)\n",
    "\n",
    "# numpy_satellite_data = np.random.random((10, 2, 10_000, 10_000))\n",
    "\n",
    "# Разделяем каналы\n",
    "# nir = numpy_satellite_data[:, 0, :, :]  # Near Infrared\n",
    "# red = numpy_satellite_data[:, 1, :, :]  # Red\n",
    "\n",
    "# Вычисляем NDVI\n",
    "# try:\n",
    "    # ndvi = (nir - red) / (nir + red)\n",
    "    # print(\"NDVI сцены 1, пиксель [0, 0]:\", ndvi[0, 0, 0])\n",
    "# except MemoryError:\n",
    "    # print(\"NumPy не справился: данные слишком велики для оперативной памяти.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d3988ce4eef82",
   "metadata": {},
   "source": [
    "Таким образом, мы посмотрели как работают массивы в Dask. Теперь потыкаем Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958d805",
   "metadata": {},
   "source": [
    "## Dask Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded21455",
   "metadata": {},
   "source": [
    "Dask Bag — это компонент библиотеки Dask, предназначенный для параллельной обработки коллекций произвольных Python объектов с использованием операций, таких как map, filter, fold и groupby. Он эффективно работает с неструктурированными или полуструктурированными данными, такими как текстовые файлы, лог-файлы, JSON-записи или пользовательские объекты Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd25dd3",
   "metadata": {},
   "source": [
    "Рассмотрим задачу подсчета наиболее частых слов в наборе текстовых файлов (например, анализ лог-файлов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1814452ad15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "from dask.distributed import Client\n",
    "import dask.bag as db\n",
    "\n",
    "# Запуск Dask Dashboard\n",
    "client = Client()\n",
    "print(client.dashboard_link)\n",
    "\n",
    "# Функция для генерации случайной строки логов\n",
    "def generate_log_line():\n",
    "    ip = \".\".join(str(random.randint(0, 255)) for _ in range(4))  # Генерация случайного IP-адреса\n",
    "    timestamp = datetime.now().strftime('%d/%b/%Y:%H:%M:%S')  # Текущее время в формате логов\n",
    "    http_methods = [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"]\n",
    "    method = random.choice(http_methods)  # Случайный HTTP-метод\n",
    "    url = \"/\" + \"/\".join(\n",
    "        ''.join(random.choices(string.ascii_lowercase, k=random.randint(3, 10))) for _ in range(3)\n",
    "    )  # Случайный URL\n",
    "    status = random.choice([200, 201, 400, 404, 500])  # Случайный статус ответа\n",
    "    response_time = random.uniform(0.1, 5.0)  # Время ответа в секундах\n",
    "\n",
    "    return f'{ip} - - [{timestamp}] \"{method} {url} HTTP/1.1\" {status} {int(response_time * 1000)}'\n",
    "\n",
    "# Обработка строк логов: выделение частей лога для анализа\n",
    "def process_log_line(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 9:  # Проверка на валидность строки лога\n",
    "        return []\n",
    "    ip = parts[0]  # IP-адрес\n",
    "    method = parts[5].strip('\"')  # HTTP-метод\n",
    "    status = parts[8]  # Статус ответа\n",
    "    return [ip, method, status]\n",
    "\n",
    "# Генерация 1 миллиона строк логов с ленивым генератором\n",
    "num_lines = 1 * 10 ** 6\n",
    "chunk_size = 1 * 10 ** 5\n",
    "data = db.from_sequence((generate_log_line() for _ in range(num_lines)), npartitions=num_lines // chunk_size)\n",
    "\n",
    "# Обработка логов\n",
    "processed_data = data.map(process_log_line).flatten()  # Обрабатываем строки логов\n",
    "\n",
    "# Подсчитываем частоту каждого элемента\n",
    "element_counts = processed_data.frequencies()\n",
    "\n",
    "# Получаем топ-10 самых частых элементов\n",
    "top_10_elements = element_counts.topk(10, key=lambda x: x[1])\n",
    "print(\"Топ-10 самых частых элементов:\", top_10_elements.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978f36e",
   "metadata": {},
   "source": [
    "Dask Bag напоминает параллельную версию библиотеки PyToolz или Python-эквивалент RDD из Apache Spark. Благодаря ленивой обработке и использованию итераторов, Dask Bag позволяет работать с данными, превышающими объем оперативной памяти, и эффективно задействовать ресурсы нескольких ядер или даже машин."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77afd6ea6532be6",
   "metadata": {},
   "source": [
    "## Как проверить прогресс выполнения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe7b2abd2de956",
   "metadata": {},
   "source": [
    "Dask поддерживает визуализацию выполнения задач через инструмент Dask Dashboard. Вы можете запустить его, добавив следующие строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557bdc9facf8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# client = Client()\n",
    "print(client.dashboard_link)  # Откроет ссылку на дашборд"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9163f0a0322ab02",
   "metadata": {},
   "source": [
    "После запуска вы сможете видеть статус выполнения задач в режиме реального времени."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b810fb8",
   "metadata": {},
   "source": [
    "## Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973da45",
   "metadata": {},
   "source": [
    "Dask DataFrame — это инструмент для работы с большими табличными данными, который позволяет масштабировать вычисления на основе библиотеки pandas. Если данные слишком велики для обработки в памяти или вычисления занимают слишком много времени, Dask DataFrame предоставляет решение, которое позволяет эффективно использовать ресурсы компьютера или распределенного кластера."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bef24f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55166968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc0ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49500171",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cda5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
